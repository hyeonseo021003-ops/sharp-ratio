import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from torch.utils.data import TensorDataset, DataLoader

# ë°ì´í„° ì¹¼ëŸ¼ëª… ìˆ˜ì •

df = pd.read_csv("distribution_model_MLP.csv")

# 1. ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼ëª…ì— ë§ê²Œ ëª…ë‹¨ ìƒì„±
# 0th_2th_prob, 2th_4th_prob, ..., 98th_100th_prob (ì´ 50ê°œ)
prob_cols = [f'{i}th_{i+2}th_prob' for i in range(0, 100, 2)]

# íŠ¹ì„±(X)ìœ¼ë¡œ ì“¸ 51ê°œ: í™•ë¥ ë¶„í¬ 50ê°œ + êµ­ì±„ìˆ˜ìµë¥  1ê°œ
feature_cols = prob_cols + ['Bond']

# íƒ€ê²Ÿ(Y)ìœ¼ë¡œ ì“¸ 1ê°œ: ì‹¤ì œ ìˆ˜ìµë¥ 
target_col = ['Return']

# ì „ì²´ ì‚¬ìš©í•  ì»¬ëŸ¼ í†µí•©
all_cols = feature_cols + target_col

# 2. ì›ë³¸ dfì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ ë³µì‚¬ë³¸ ìƒì„±
# 24GB ë¨ì„ í™œìš©í•´ ì•ˆì „í•˜ê²Œ .copy()ë¡œ ë³µì œ
df_copy = df[all_cols].copy()


#------------------------------------------------------------------------------

# ìµœì í™” ë° í•™ìŠµ (GPU ìµœì í™” ë²„ì „)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



model = nn.Sequential(
    nn.Linear(51, 256),
    nn.LeakyReLU(negative_slope=0.01),

    nn.Linear(256, 256),
    nn.LeakyReLU(negative_slope=0.01),

    nn.Linear(256, 256),
    nn.LeakyReLU(negative_slope=0.01),

    nn.Linear(256, 256),
    nn.LeakyReLU(negative_slope=0.01),

    nn.Linear(256, 2), # ì¶œë ¥ì¸µ: 64ê°œ ì •ë³´ë¥¼ ëª¨ì•„ ìµœì¢…ì ìœ¼ë¡œ 1ê°œì˜ 'ì ìˆ˜' ë„ì¶œ
    nn.Softmax(dim=-1) # Sigmoid ëŒ€ì‹  Softmax ì‚¬ìš©
).to(device)


# ìµœì í™” ë„êµ¬: Adamì„ ì‚¬ìš©í•˜ë©°, í•™ìŠµë¥ (lr)ì€ 0.001ë¡œ ì„¤ì •
optimizer = optim.Adam(model.parameters(), lr=0.001)





# train data set ì¤€ë¹„

# 0. ì„¤ì • ê°’ ì •ì˜ 
PER_SET = 1000
TRAIN_SIZE = 300000  # 30íŒ€
VAL_SIZE = 200000     # 20íŒ€
TEST_SIZE = 200000    # 20íŒ€

# ì‹¤ì œ íŒ€ ìˆ˜ ê³„ì‚° 
train_sets = TRAIN_SIZE // PER_SET  # 30íŒ€
val_sets = VAL_SIZE // PER_SET 
test_sets = TEST_SIZE // PER_SET 

# 1. ì¸ë±ìŠ¤ ë¶„ë¦¬ (ì¤‘ë³µ ë°©ì§€)
all_indices = np.arange(len(df_copy))

# í•™ìŠµìš© ì¶”ì¶œ
train_idx = np.random.choice(all_indices, size=TRAIN_SIZE, replace=False)

# í•™ìŠµìš© ì œì™¸í•œ ë‚˜ë¨¸ì§€ np.setdiff1d: ì°¨ì§‘í•©
remaining_idx = np.setdiff1d(all_indices, train_idx)

# ë‚˜ë¨¸ì§€ ì¤‘ ê²€ì¦ìš© ì¶”ì¶œ
val_idx = np.random.choice(remaining_idx, size=VAL_SIZE, replace=False)

# ë‚˜ë¨¸ì§€ ì¤‘ í…ŒìŠ¤íŠ¸ìš© ì¶”ì¶œ
remaining_idx = np.setdiff1d(remaining_idx, val_idx)
test_idx = np.random.choice(remaining_idx, size=TEST_SIZE, replace=False)

# 2. 3D í…ì„œ ë³€í™˜ í•¨ìˆ˜ (ë°˜ë³µ ì‘ì—… ìµœì í™”)
def make_3d_tensor(indices, num_sets):
    # ë°ì´í„° ì¶”ì¶œ ë° Reshape (íŒ€ìˆ˜, íŒ€ë‹¹ì¸ì›, ì»¬ëŸ¼ìˆ˜)
    data_np = df_copy.iloc[indices].values
    data_3d = data_np.reshape(num_sets, PER_SET, 52)
    
    # í…ì„œ ë³€í™˜ ë° ì¥ì¹˜ ì´ë™
    tensor = torch.tensor(data_3d, dtype=torch.float32).to(device)
    
    # X(í”¼ì²˜ 51ê°œ)ì™€ Y(íƒ€ê²Ÿ 1ê°œ) ë¶„ë¦¬
    x = tensor[:, :, :51]
    y = tensor[:, :, 51:]
    return x, y

# 3. ìµœì¢… ë°ì´í„° ìƒì„±
train_x, train_y = make_3d_tensor(train_idx, train_sets)
val_x, val_y     = make_3d_tensor(val_idx, val_sets)
test_x, test_y   = make_3d_tensor(test_idx, test_sets)


batch_size = 34  # í•œ ë²ˆì— ëª‡ 'íŒ€'ì„ í•™ìŠµí• ì§€ 
train_dataset = TensorDataset(train_x, train_y)  # train_x: (T, N, 51), train_y: (T, N, 1)
val_dataset   = TensorDataset(val_x, val_y)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)

# ê²°ê³¼ í™•ì¸
print("âœ…")








def criterion(probs_3d, x_3d, y_3d):
    # x_3dì˜ ë§ˆì§€ë§‰ ì»¬ëŸ¼(-1)ì´ êµ­ì±„ ìˆ˜ìµë¥ (Bond1)ì´ë¼ê³  ê°€ì •
    r_tsy_3d = x_3d[:, :, -1]          
    actual_ret_3d = y_3d.squeeze(-1)   # ì‹¤ì œ ìˆ˜ìµë¥  (3Dë¡œ ë§ì¶¤)
    
    # ì „ëµ ìˆ˜ìµë¥  ê³„ì‚° (ëª¨ë¸ì´ ìŠ¹ì¸í•˜ë©´ ì‹¤ì œ ìˆ˜ìµë¥ , ê±°ì ˆí•˜ë©´ êµ­ì±„ ìˆ˜ìµë¥ )
    # ëê¹Œì§€ ìŠ¤ë¬´ìŠ¤í•˜ê²Œ í•˜ëŠ”ê±´?
    individual_returns = (probs_3d * actual_ret_3d) + ((1 - probs_3d) * r_tsy_3d)
    
    # íŒ€ë³„(Batch) í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
    mean_ret = individual_returns.mean(dim=1)
    std_ret = individual_returns.std(dim=1)
    mean_rtsy = r_tsy_3d.mean(dim=1)
    
    # ìƒ¤í”„ ì§€ìˆ˜ ê³„ì‚° (ë¶„ëª¨ì— ì•„ì£¼ ì‘ì€ ê°’ 1e-8ì„ ë”í•´ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
    batch_sharpe = (mean_ret - mean_rtsy) / (std_ret + 1e-8)
    
    # LossëŠ” ë§ˆì´ë„ˆìŠ¤ ìƒ¤í”„ ì§€ìˆ˜ì˜ í‰ê·  (ì´ ê°’ì´ ë‚®ì•„ì§ˆìˆ˜ë¡ ìƒ¤í”„ ì§€ìˆ˜ëŠ” ì˜¬ë¼ê°)
    return -batch_sharpe.mean()



def train_with_early_stopping(model, train_loader, val_loader, optimizer, num_epochs=200, patience=30):
    best_val_loss = float('inf')
    best_model_state = None
    warning = 0
    train_loss_history = []
    val_loss_history = []

    print("ğŸš€ í•™ìŠµ ë° ì‹¤ì‹œê°„ ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    for epoch in range(num_epochs):
        # --- 1. í•™ìŠµ ë‹¨ê³„ (Train Step) ---
        model.train()
        for bx, by in train_loader:
            bx = bx.to(device)  # (B, N, 51)
            by = by.to(device)  # (B, N, 1)
            N = bx.size(1)  # íŒ€ë‹¹ ì¸ì› ìˆ˜ (í•˜ë“œì½”ë”© ê¸ˆì§€)

        # Forward: ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ê³  ìŠ¹ì¸ í™•ë¥  ì¶”ì¶œ
        probs_flat = model(bx.reshape(-1, bx.size(-1)))
        probs_3d = probs_flat.view(-1, N, 2)[:, :, 1]
        
        # Loss ê³„ì‚°: ìƒ¤í”„ ì§€ìˆ˜ ê¸°ë°˜ì˜ ì†ì‹¤í•¨ìˆ˜ ìµœì í™”
        loss = criterion(probs_3d, bx, by)
        
        # Optimization: ê°€ì¤‘ì¹˜ ìˆ˜ì • ì‹¤í–‰
        optimizer.zero_grad() # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        loss.backward()    # ì—­ì „íŒŒ ì‹¤í–‰
        optimizer.step()     # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        
        train_loss_history.append(loss.item())


        # --- 2. ê²€ì¦ ë‹¨ê³„ (Validation Step) ---
        model.eval()
        with torch.no_grad():
            for vx, vy in val_loader:
                vx = vx.to(device)
                vy = vy.to(device)

                N = vx.size(1)
                # ê²€ì¦ìš© ë°ì´í„°(20íŒ€)ë¡œ í˜„ì¬ ëª¨ë¸ ì‹¤ë ¥ ì¸¡ì •
                v_probs_flat = model(vx.reshape(-1, vx.size(-1)))
                v_probs_3d = v_probs_flat.view(-1, N, 2)[:, :, 1]
                v_loss = criterion(v_probs_3d, vx, vy)
            
                # ë§¤ ì—í­ì˜ ê²°ê³¼(Loss) ì €ì¥
        
                val_loss_history.append(v_loss.item())

        print(f"Epoch [{epoch+1:3d}] | Train Loss: {loss.item():.4f} | Val Loss: {v_loss.item():.4f}")


        # ìµœì  ëª¨ë¸ ì €ì¥ (ê°€ì¥ ë‚®ì€ Val Loss ê¸°ë¡ ì‹œ)
        if v_loss < best_val_loss:
            best_val_loss = v_loss
            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}
            warning = 0
            torch.save(model.state_dict(), "best_model.pth")
        else:
            warning +=1
            
        if warning >= patience:
            print(' í•™ìŠµ ì¢…ë£Œ ')
            break

   
            

    return train_loss_history, val_loss_history, best_val_loss







# test

# 1. ë°ì´í„°ë¥¼ í´ê³  ëª¨ë¸ì„ í†µê³¼ì‹œì¼œ ë³µêµ¬í•˜ëŠ” í•¨ìˆ˜
def get_model_predictions(test_x_3d, test_y_3d):
    model.eval() # í‰ê°€ëª¨ë“œ
    with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚° ê¸ˆì§€ - ë©”ëª¨ë¦¬, ì†ë„
        # (20, 10000, 51) -> (20000, 51)ë¡œ í´ê¸°
        x_flat = test_x_3d.view(-1, 51)
        
        # ëª¨ë¸ í†µê³¼ (ê²°ê³¼: 20000,)
        probs_flat = model(x_flat)
        
        # ì›ë˜ êµ¬ì¡°ë¡œ ë³µêµ¬ (ê²°ê³¼: 20, 1000)
        probs_3d_full = probs_flat.view(-1, 1000,2)
        probs_3d = probs_3d_full[:, :, 1] # ìŠ¹ì¸ í™•ë¥ ë§Œ ì¶”ì¶œ

        r_tsy_3d = test_x_3d[:, :, -1] # êµ­ì±„ ìˆ˜ìµë¥  ë¶„ë¦¬
        actual_ret_3d = test_y_3d.squeeze(-1) # ì‹¤ì œ ìˆ˜ìµë¥ 

    # dim=-1ì€ ë§ˆì§€ë§‰ ì°¨ì›(2ê°œ ì ìˆ˜) ì¤‘ í° ì¸ë±ìŠ¤ë¥¼ ê³ ë¦„
    #        = (probs_3d >= 0.5).float() 
    decisions = torch.argmax(probs_3d_full, dim=-1).float() # ê²°ê³¼: (20, 1000)
    
    # 3. ê°œë³„ ìˆ˜ìµë¥  ê²°ì •
    # ìŠ¹ì¸(1)ì¸ ì‚¬ëŒì€ actual_loan_retì„, ë¯¸ìŠ¹ì¸(0)ì¸ ì‚¬ëŒì€ r_tsyë¥¼ ê°€ì§
    # ê°œê°œì¸ì˜ ìˆ˜ìµë¥  (20, 1000)í–‰ë ¬ì„ ê²°ê³¼ê°’ìœ¼ë¡œ
    individual_returns = (decisions * actual_ret_3d) + ((1 - decisions) * r_tsy_3d)
    
    # 4. 3000ëª…ì˜ í‰ê·  ìˆ˜ìµë¥ ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
    mean_ret = individual_returns.mean(dim=1)   
    std_ret = individual_returns.std(dim=1)     
    mean_rtsy = r_tsy_3d.mean(dim=1)
    # 5. Sharpe Ratio ê³„ì‚° => í‰ê·  ìˆ˜ìµë¥  - í‰ê·  êµ­ì±„ ìˆ˜ìµë¥  í‰ê· (ìˆ˜ìµë¥ -êµ­ì±„ìˆ˜ìµë¥ )
    final_sharpe = (mean_ret - mean_rtsy)/ (std_ret + 1e-8)
    
    return final_sharpe, decisions, mean_ret, mean_rtsy, std_ret






# 1. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
print("\n" + "="*50)
print(" [Step 1] ëª¨ë¸ í•™ìŠµ(Training) ì‹œì‘ (Early Stopping ì ìš©)...")
print("="*50)
loss_history = train_with_early_stopping(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optimizer,
    num_epochs=1000,
    patience=30
)
print(">> í•™ìŠµ ì™„ë£Œ!")

# 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²°ê³¼ ë„ì¶œ
print("\n" + "="*50)
print(" [Step 2] í…ŒìŠ¤íŠ¸ ë°ì´í„°(Test Data) í‰ê°€ ì‹œì‘...")
print("="*50)
sharpe_results, decisions, mean_ret, mean_rtsy, std_ret = get_model_predictions(test_x, test_y)



# 3. ìƒì„¸ ìˆ˜ì¹˜ ê³„ì‚° (í…ì„œë¥¼ ë„˜íŒŒì´ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°)
avg_sharpe = sharpe_results.mean().item()
max_sharpe = sharpe_results.max().item()
min_sharpe = sharpe_results.min().item()

total_test_count = decisions.numel()
approved_count = int(decisions.sum().item())
approval_rate = (approved_count / total_test_count) * 100
approved_returns = test_y.squeeze(-1).cpu()[decisions.cpu() == 1]


avg_approved_return = approved_returns.mean().item() * 100


# ê²°ê³¼ ì¶œë ¥
print("\n" + "*"*20 + " [ ìµœì¢… ì„±ì í‘œ ] " + "*"*20)
print(f"â–¶ ì „ì²´ í…ŒìŠ¤íŠ¸ ì¸ì›      : {total_test_count:,} ëª…")
print(f"â–¶ ëª¨ë¸ì´ ìŠ¹ì¸í•œ ì¸ì›    : {approved_count:,} ëª…")
print(f"â–¶ ìµœì¢… ìŠ¹ì¸ìœ¨          : {approval_rate:.2f} %")
print("-" * 51)
print(f"â–¶ í‰ê·  ìƒ¤í”„ ì§€ìˆ˜(Sharpe) : {avg_sharpe:.4f}")
print(f"â–¶ ìµœê³  íŒ€ ìƒ¤í”„ ì§€ìˆ˜     : {max_sharpe:.4f}")
print(f"â–¶ ìµœì € íŒ€ ìƒ¤í”„ ì§€ìˆ˜     : {min_sharpe:.4f}")
print("-" * 51)
print(f"â–¶ ìŠ¹ì¸ëœ ëŒ€ì¶œì˜ í‰ê·  ìˆ˜ìµë¥  : {avg_approved_return:.2f} %")
print("â–¶ ì „ëµ í‰ê· ìˆ˜ìµ(mean_ret) í‰ê· :", mean_ret.mean().item())
print("â–¶ êµ­ì±„ í‰ê· ìˆ˜ìµ(mean_rtsy) í‰ê· :", mean_rtsy.mean().item())
print("â–¶ ì „ëµ í‘œì¤€í¸ì°¨(std_ret) í‰ê· :", std_ret.mean().item())
print("â–¶ ìƒ¤í”„(final_sharpe) í‰ê· :", sharpe_results.mean().item())
print("*"*54)




print("ìŠ¹ì¸ í‘œë³¸ ìˆ˜:", approved_returns.numel())
print("ìŠ¹ì¸ í‰ê· (ì›ê°’):", approved_returns.mean().item())
print("ìŠ¹ì¸ í‰ê· (%):", approved_returns.mean().item() * 100)

