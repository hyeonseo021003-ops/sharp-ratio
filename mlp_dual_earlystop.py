import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ì¹¼ëŸ¼ëª… ìˆ˜ì •

df = pd.read_csv("distribution_model_MLP.csv")

# 1. ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼ëª…ì— ë§ê²Œ ëª…ë‹¨ ìƒì„±
# 0th_2th_prob, 2th_4th_prob, ..., 98th_100th_prob (ì´ 50ê°œ)
prob_cols = [f'{i}th_{i+2}th_prob' for i in range(0, 100, 2)]

# íŠ¹ì„±(X)ìœ¼ë¡œ ì“¸ 51ê°œ: í™•ë¥ ë¶„í¬ 50ê°œ + êµ­ì±„ìˆ˜ìµë¥  1ê°œ
feature_cols = prob_cols + ['Bond']

# íƒ€ê²Ÿ(Y)ìœ¼ë¡œ ì“¸ 1ê°œ: ì‹¤ì œ ìˆ˜ìµë¥ 
target_col = ['Return']

# ì „ì²´ ì‚¬ìš©í•  ì»¬ëŸ¼ í†µí•©
all_cols = feature_cols + target_col

# 2. ì›ë³¸ dfì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ ë³µì‚¬ë³¸ ìƒì„±
# 24GB ë¨ì„ í™œìš©í•´ ì•ˆì „í•˜ê²Œ .copy()ë¡œ ë³µì œí•©ë‹ˆë‹¤.
df_copy = df[all_cols].copy()

# 3. í˜¹ì‹œ ëª¨ë¥¼ ê²°ì¸¡ì¹˜(NaN) 0ìœ¼ë¡œ ì±„ìš°ê¸°
df_copy = df_copy.fillna(0)


feature_cols = [col for col in df_copy.columns if col != 'Return']

#------------------------------------------------------------------------------

# ì •ê·œí™”
# 2. ìŠ¤ì¼€ì¼ë§ ì ìš©

# ìµœì í™” ë° í•™ìŠµ (GPU ìµœì í™” ë²„ì „)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



model = nn.Sequential(
    nn.Linear(51, 64),
    nn.LeakyReLU(negative_slope=0.01),

    nn.Linear(64, 64),
    nn.LeakyReLU(negative_slope=0.01),

    nn.Linear(64, 64),
    nn.LeakyReLU(negative_slope=0.01),

    nn.Linear(64, 64),
    nn.LeakyReLU(negative_slope=0.01),

    nn.Linear(64, 2), # ì¶œë ¥ì¸µ: 64ê°œ ì •ë³´ë¥¼ ëª¨ì•„ ìµœì¢…ì ìœ¼ë¡œ 1ê°œì˜ 'ì ìˆ˜' ë„ì¶œ
    nn.Softmax(dim=-1) # Sigmoid ëŒ€ì‹  Softmax ì‚¬ìš©
).to(device)

###### ì—¬ê¸°ì— ìë¹„ì–´ ë„£ê¸°

# ìµœì í™” ë„êµ¬: Adamì„ ì‚¬ìš©í•˜ë©°, í•™ìŠµë¥ (lr)ì€ 0.001ë¡œ ì„¤ì •
optimizer = optim.Adam(model.parameters(), lr=0.001)





# train data set ì¤€ë¹„

# 0. ì„¤ì • ê°’ ì •ì˜
PER_SET = 10000
TRAIN_SIZE = 600000  # 100íŒ€
VAL_SIZE = 200000     # 15íŒ€ (5ë§Œ ê·¼ì‚¬ì¹˜ ìœ„í•´ 3000ì˜ ë°°ìˆ˜ì¸ 45000 ì„ íƒí•˜ê±°ë‚˜, ì•„ë˜ì²˜ëŸ¼ ìë™ ê³„ì‚°)
TEST_SIZE = 200000    # 15íŒ€

# ì‹¤ì œ íŒ€ ìˆ˜ ê³„ì‚° 
train_sets = TRAIN_SIZE // PER_SET  # 100íŒ€
val_sets = VAL_SIZE // PER_SET 
test_sets = TEST_SIZE // PER_SET 

# 1. ì¸ë±ìŠ¤ ë¶„ë¦¬ (ì¤‘ë³µ ë°©ì§€)
all_indices = np.arange(len(df_copy))

# í•™ìŠµìš© ì¶”ì¶œ
train_idx = np.random.choice(all_indices, size=TRAIN_SIZE, replace=False)

# í•™ìŠµìš© ì œì™¸í•œ ë‚˜ë¨¸ì§€
remaining_idx = np.setdiff1d(all_indices, train_idx)

# ë‚˜ë¨¸ì§€ ì¤‘ ê²€ì¦ìš© ì¶”ì¶œ
val_idx = np.random.choice(remaining_idx, size=val_sets * PER_SET, replace=False)

# ë‚˜ë¨¸ì§€ ì¤‘ í…ŒìŠ¤íŠ¸ìš© ì¶”ì¶œ
remaining_idx = np.setdiff1d(remaining_idx, val_idx)
test_idx = np.random.choice(remaining_idx, size=test_sets * PER_SET, replace=False)

# 2. 3D í…ì„œ ë³€í™˜ í•¨ìˆ˜ (ë°˜ë³µ ì‘ì—… ìµœì í™”)
def make_3d_tensor(indices, num_sets):
    # ë°ì´í„° ì¶”ì¶œ ë° Reshape (íŒ€ìˆ˜, íŒ€ë‹¹ì¸ì›, ì»¬ëŸ¼ìˆ˜)
    data_np = df_copy.iloc[indices].values
    data_3d = data_np.reshape(num_sets, PER_SET, 52)
    
    # í…ì„œ ë³€í™˜ ë° ì¥ì¹˜ ì´ë™
    tensor = torch.tensor(data_3d, dtype=torch.float32).to(device)
    
    # X(í”¼ì²˜ 51ê°œ)ì™€ Y(íƒ€ê²Ÿ 1ê°œ) ë¶„ë¦¬
    x = tensor[:, :, :51]
    y = tensor[:, :, 51:]
    return x, y

# 3. ìµœì¢… ë°ì´í„° ìƒì„±
train_x, train_y = make_3d_tensor(train_idx, train_sets)
val_x, val_y     = make_3d_tensor(val_idx, val_sets)
test_x, test_y   = make_3d_tensor(test_idx, test_sets)

# ê²°ê³¼ í™•ì¸
print("âœ…")








def criterion(probs_3d, x_3d, y_3d):
    # x_3dì˜ ë§ˆì§€ë§‰ ì»¬ëŸ¼(-1)ì´ êµ­ì±„ ìˆ˜ìµë¥ (Bond1)ì´ë¼ê³  ê°€ì •
    r_tsy_3d = x_3d[:, :, -1]          
    actual_ret_3d = y_3d.squeeze(-1)   # ì‹¤ì œ ìˆ˜ìµë¥  (3Dë¡œ ë§ì¶¤)
    
    # ì „ëµ ìˆ˜ìµë¥  ê³„ì‚° (ëª¨ë¸ì´ ìŠ¹ì¸í•˜ë©´ ì‹¤ì œ ìˆ˜ìµë¥ , ê±°ì ˆí•˜ë©´ êµ­ì±„ ìˆ˜ìµë¥ )
    individual_returns = (probs_3d * actual_ret_3d) + ((1 - probs_3d) * r_tsy_3d)
    
    # íŒ€ë³„(Batch) í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
    mean_ret = individual_returns.mean(dim=1)
    std_ret = individual_returns.std(dim=1)
    mean_rtsy = r_tsy_3d.mean(dim=1)
    
    # ìƒ¤í”„ ì§€ìˆ˜ ê³„ì‚° (ë¶„ëª¨ì— ì•„ì£¼ ì‘ì€ ê°’ 1e-8ì„ ë”í•´ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
    batch_sharpe = (mean_ret - mean_rtsy) / (std_ret + 1e-8)
    
    # LossëŠ” ë§ˆì´ë„ˆìŠ¤ ìƒ¤í”„ ì§€ìˆ˜ì˜ í‰ê·  (ì´ ê°’ì´ ë‚®ì•„ì§ˆìˆ˜ë¡ ìƒ¤í”„ ì§€ìˆ˜ëŠ” ì˜¬ë¼ê°)
    return -batch_sharpe.mean()



def train_with_early_stopping(train_x, train_y, val_x, val_y, num_epochs=1000, patience=30):
    best_val_loss = float('inf')
    best_model_state = None
    warning = 0
    train_loss_history = []
    val_loss_history = []

    print("ğŸš€ í•™ìŠµ ë° ì‹¤ì‹œê°„ ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    for epoch in range(num_epochs):
        # --- 1. í•™ìŠµ ë‹¨ê³„ (Train Step) ---
        model.train()
        
        # Forward: ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ê³  ìŠ¹ì¸ í™•ë¥  ì¶”ì¶œ
        t_probs_flat = model(train_x.view(-1, 51))
        t_probs_3d = t_probs_flat.view(-1, 10000, 2)[:, :, 1]
        
        # Loss ê³„ì‚°: ìƒ¤í”„ ì§€ìˆ˜ ê¸°ë°˜ì˜ ì†ì‹¤í•¨ìˆ˜ ìµœì í™”
        t_loss = criterion(t_probs_3d, train_x, train_y)
        
        # Optimization: ê°€ì¤‘ì¹˜ ìˆ˜ì • ì‹¤í–‰
        optimizer.zero_grad() # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        t_loss.backward()    # ì—­ì „íŒŒ ì‹¤í–‰
        optimizer.step()     # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        
        # --- 2. ê²€ì¦ ë‹¨ê³„ (Validation Step) ---
        model.eval()
        with torch.no_grad():
            # ê²€ì¦ìš© ë°ì´í„°(20íŒ€)ë¡œ í˜„ì¬ ëª¨ë¸ ì‹¤ë ¥ ì¸¡ì •
            v_probs_flat = model(val_x.view(-1, 51))
            v_probs_3d = v_probs_flat.view(-1, 10000, 2)[:, :, 1]
            v_loss = criterion(v_probs_3d, val_x, val_y)
            
        # ë§¤ ì—í­ì˜ ê²°ê³¼(Loss) ì €ì¥
        train_loss_history.append(t_loss.item())
        val_loss_history.append(v_loss.item())

        print(f"Epoch [{epoch+1:3d}] | Train Loss: {t_loss.item():.4f} | Val Loss: {v_loss.item():.4f}")


        # ìµœì  ëª¨ë¸ ì €ì¥ (ê°€ì¥ ë‚®ì€ Val Loss ê¸°ë¡ ì‹œ)
        if v_loss < best_val_loss:
            best_val_loss = v_loss
            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}
            warning = 0
        else:
            warning +=1
            
        if warning >= patience:
            print(' í•™ìŠµ ì¢…ë£Œ ')
            break

        # best ëª¨ë¸ ë³µì›
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    
            

    return train_loss_history, val_loss_history







# test

# 1. ë°ì´í„°ë¥¼ í´ê³  ëª¨ë¸ì„ í†µê³¼ì‹œì¼œ ë³µêµ¬í•˜ëŠ” í•¨ìˆ˜
def get_model_predictions(test_x_3d, test_y_3d):
    model.eval() # í‰ê°€ëª¨ë“œ
    with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚° ê¸ˆì§€ - ë©”ëª¨ë¦¬, ì†ë„
        # (20, 10000, 51) -> (200000, 51)ë¡œ í´ê¸°
        x_flat = test_x_3d.view(-1, 51)
        
        # ëª¨ë¸ í†µê³¼ (ê²°ê³¼: 200000,)
        probs_flat = model(x_flat)
        
        # ì›ë˜ êµ¬ì¡°ë¡œ ë³µêµ¬ (ê²°ê³¼: 20, 10000)
        probs_3d_full = probs_flat.view(-1, 10000,2)
        probs_3d = probs_3d_full[:, :, 1] # ìŠ¹ì¸ í™•ë¥ ë§Œ ì¶”ì¶œ

        r_tsy_3d = test_x_3d[:, :, -1] # êµ­ì±„ ìˆ˜ìµë¥  ë¶„ë¦¬
        actual_ret_3d = test_y_3d.squeeze(-1) # ì‹¤ì œ ìˆ˜ìµë¥ 

    # ë°©ë²• A: argmax ì‚¬ìš© (ë‘˜ ì¤‘ í° ê²ƒ ì„ íƒ, threshold ë¬´ì‹œë¨)
    # dim=-1ì€ ë§ˆì§€ë§‰ ì°¨ì›(2ê°œ ì ìˆ˜) ì¤‘ í° ì¸ë±ìŠ¤ë¥¼ ê³ ë¦„
    #        = (probs_3d >= 0.5).float() 
    decisions = torch.argmax(probs_3d_full, dim=-1).float() # ê²°ê³¼: (20, 10000)
    
    # 3. ê°œë³„ ìˆ˜ìµë¥  ê²°ì •
    # ìŠ¹ì¸(1)ì¸ ì‚¬ëŒì€ actual_loan_retì„, ë¯¸ìŠ¹ì¸(0)ì¸ ì‚¬ëŒì€ r_tsyë¥¼ ê°€ì§
    # ê°œê°œì¸ì˜ ìˆ˜ìµë¥  (20, 10000)í–‰ë ¬ì„ ê²°ê³¼ê°’ìœ¼ë¡œ
    individual_returns = (decisions * actual_ret_3d) + ((1 - decisions) * r_tsy_3d)
    
    # 4. 3000ëª…ì˜ í‰ê·  ìˆ˜ìµë¥ ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
    mean_ret = individual_returns.mean(dim=1)   
    std_ret = individual_returns.std(dim=1)     
    mean_rtsy = r_tsy_3d.mean(dim=1)
    # 5. Sharpe Ratio ê³„ì‚° => í‰ê·  ìˆ˜ìµë¥  - í‰ê·  êµ­ì±„ ìˆ˜ìµë¥  í‰ê· (ìˆ˜ìµë¥ -êµ­ì±„ìˆ˜ìµë¥ )
    final_sharpe = (mean_ret - mean_rtsy)/ (std_ret + 1e-8)
    
    return final_sharpe, decisions, mean_ret, mean_rtsy, std_ret






# 1. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
print("\n" + "="*50)
print(" [Step 1] ëª¨ë¸ í•™ìŠµ(Training) ì‹œì‘ (Early Stopping ì ìš©)...")
print("="*50)
loss_history = train_with_early_stopping(
    train_x,  
    train_y, 
    val_x,    
    val_y, 
    num_epochs=300)
print(">> í•™ìŠµ ì™„ë£Œ!")

# 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²°ê³¼ ë„ì¶œ
print("\n" + "="*50)
print(" [Step 2] í…ŒìŠ¤íŠ¸ ë°ì´í„°(Test Data) í‰ê°€ ì‹œì‘...")
print("="*50)
sharpe_results, decisions, mean_ret, mean_rtsy, std_ret = get_model_predictions(test_x, test_y)



# 3. ìƒì„¸ ìˆ˜ì¹˜ ê³„ì‚° (í…ì„œë¥¼ ë„˜íŒŒì´ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°)
avg_sharpe = sharpe_results.mean().item()
max_sharpe = sharpe_results.max().item()
min_sharpe = sharpe_results.min().item()

total_test_count = decisions.numel()
approved_count = int(decisions.sum().item())
approval_rate = (approved_count / total_test_count) * 100
approved_returns = test_y.squeeze(-1).cpu()[decisions.cpu() == 1]


avg_approved_return = approved_returns.mean().item() * 100


# ê²°ê³¼ ì¶œë ¥
print("\n" + "*"*20 + " [ ìµœì¢… ì„±ì í‘œ ] " + "*"*20)
print(f"â–¶ ì „ì²´ í…ŒìŠ¤íŠ¸ ì¸ì›      : {total_test_count:,} ëª…")
print(f"â–¶ ëª¨ë¸ì´ ìŠ¹ì¸í•œ ì¸ì›    : {approved_count:,} ëª…")
print(f"â–¶ ìµœì¢… ìŠ¹ì¸ìœ¨          : {approval_rate:.2f} %")
print("-" * 51)
print(f"â–¶ í‰ê·  ìƒ¤í”„ ì§€ìˆ˜(Sharpe) : {avg_sharpe:.4f}")
print(f"â–¶ ìµœê³  íŒ€ ìƒ¤í”„ ì§€ìˆ˜     : {max_sharpe:.4f}")
print(f"â–¶ ìµœì € íŒ€ ìƒ¤í”„ ì§€ìˆ˜     : {min_sharpe:.4f}")
print("-" * 51)
print(f"â–¶ ìŠ¹ì¸ëœ ëŒ€ì¶œì˜ í‰ê·  ìˆ˜ìµë¥  : {avg_approved_return:.2f} %")
print("â–¶ ì „ëµ í‰ê· ìˆ˜ìµ(mean_ret) í‰ê· :", mean_ret.mean().item())
print("â–¶ êµ­ì±„ í‰ê· ìˆ˜ìµ(mean_rtsy) í‰ê· :", mean_rtsy.mean().item())
print("â–¶ ì „ëµ í‘œì¤€í¸ì°¨(std_ret) í‰ê· :", std_ret.mean().item())
print("â–¶ ìƒ¤í”„(final_sharpe) í‰ê· :", sharpe_results.mean().item())
print("*"*54)




print("ìŠ¹ì¸ í‘œë³¸ ìˆ˜:", approved_returns.numel())
print("ìŠ¹ì¸ í‰ê· (ì›ê°’):", approved_returns.mean().item())
print("ìŠ¹ì¸ í‰ê· (%):", approved_returns.mean().item() * 100)

