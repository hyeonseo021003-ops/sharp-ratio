# 스케일링을 적용할 칼럼 정의
columns_to_scale = {
    'StandardScaler_columns': ['dti', 'fico_range', 'sub_grade', 'installment', 'Bond', 'mo_sin_old_rev_tl_op', 'revol_util', 'Installment_to_annual_inc', 'spread', 'int_rate_100'],  # 예시: StandardScaler를 적용할 칼럼
    'MinMaxScaler_columns': ['acc_now_delinq', 'inq_last_6mths', 'num_accts_ever_120_pd', 'num_tl_90g_dpd_24m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec', 'pub_rec_bankruptcies', 'total_bal_ex_mort'],  # 예시: MinMaxScaler를 적용할 칼럼
    'QuantileTransformer_columns': ['annual_inc', 'funded_amnt'],  # 예시: QuantileTransformer를 적용할 칼럼
    'No_Scaling_columns': ['application_type', 'term', 'is_Verified', 'is_Source_Verified', 'is_Not_Verified']  # 예시: 스케일링하지 않을 칼럼
}# 학습 데이터와 검증 데이터 분리

# StandardScaler, MinMaxScaler, QuantileTransformer 정의
scalers = {
    'StandardScaler': StandardScaler(),
    'MinMaxScaler': MinMaxScaler(),
    'QuantileTransformer': QuantileTransformer(output_distribution='normal')  # 정규분포로 변환
}

# ColumnTransformer로 각 칼럼에 맞는 스케일링 적용
preprocessor = ColumnTransformer(
    transformers=[
        ('standard', scalers['StandardScaler'], columns_to_scale['StandardScaler_columns']),
        ('minmax', scalers['MinMaxScaler'], columns_to_scale['MinMaxScaler_columns']),
        ('quantile', scalers['QuantileTransformer'], columns_to_scale['QuantileTransformer_columns']),
        ('no_scale', 'passthrough', columns_to_scale['No_Scaling_columns'])
    ])






















X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.dtypes)
print("====")
print(y_train.dtypes)

# 데이터 로더 설정
X_train = preprocessor.fit_transform(X_train)
X_val = preprocessor.transform(X_val)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).view(-1)  # 1D로 변환
y_val_tensor = torch.tensor(y_val.values, dtype=torch.long).view(-1)  # 1D로 변환

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)


train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# MLP 모델 설계
class MLPModel(nn.Module):
    def __init__(self, input_dim, num_classes=50):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, num_classes)  # 50개의 클래스로 분류
        
    def forward(self, x):
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        x = F.leaky_relu(self.fc3(x))
        x = self.fc4(x)  # 출력층
        return x

        # 1. 데이터 로더 설정
batch_size = 64  # 배치 크기

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)


# 모델 초기화
model = MLPModel(input_dim=X_train_tensor.shape[1], num_classes=50)
model = model.to(torch.device('cuda'))
# 손실 함수 (CrossEntropyLoss는 내부적으로 softmax를 포함하므로 log_softmax는 필요없음)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

num_epochs = 100
patience = 9


# 4. 학습 함수 정의
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10):
    best_val_loss = float('inf')
    epochs_without_improvement = 0
    
    # 학습 루프
    for epoch in range(num_epochs):
        model.train()  # 모델을 학습 모드로 설정
        train_loss = 0.0
        correct = 0
        total = 0
        
        # 훈련 데이터에 대해 배치 단위로 학습
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(torch.device('cuda')), labels.to(torch.device('cuda'))
            
            # 옵티마이저 초기화
            optimizer.zero_grad()
            
            # 순전파
            outputs = model(inputs)
            labels = labels.squeeze()
            
            loss = criterion(outputs, labels)
            
            # 역전파 및 최적화
            loss.backward()
            optimizer.step()
            
            # 훈련 손실 계산
            train_loss += loss.item()
            
            # 예측 값 계산
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
        
        # 훈련 손실 및 정확도 계산
        train_loss /= len(train_loader)
        train_accuracy = 100 * correct / total
        
        # 검증 루프
        model.eval()  # 모델을 평가 모드로 설정
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():  # 검증 시에는 gradient를 계산하지 않음
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(torch.device('cuda')), labels.to(torch.device('cuda'))
                
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
        
        # 검증 손실 및 정확도 계산
        val_loss /= len(val_loader)
        val_accuracy = 100 * correct / total
        
        print(f'Epoch {epoch+1}/{num_epochs}')
        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')
        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')

        # Early stopping 기준
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_without_improvement = 0
            # 모델 저장
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= patience:
                print("Early stopping...")
                break


# 5. 모델 훈련
train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10)